#!/usr/bin/env python
# elephant
"""
Elephant is a statistical sentence and word boundaries detection system.
Wapiti needs to be installed and in the path in order to use elephant.
"""

from optparse import OptionParser
import sys
import os
from subprocess import Popen, PIPE
from tempfile import mkstemp
import codecs                # For utf-8 output
import unicodedata as ud     
import itertools

def paras(src):
    "Splits src into paragraphs separated by empty lines"
    current = []
    for line in src:
        if line.strip() == "":
            yield current
            current = []
        else: 
            current.append(line)
    if current:
        yield current

def is_exe(fpath):
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

def check_exec(program):
    """Checks if 'program' exists in the path and it is executable."""
    
    fpath, fname = os.path.split(program)
    if fpath:
        if is_exe(program):
            return True
    else:
        for path in os.environ["PATH"].split(os.pathsep):
            path = path.strip('"')
            exe_file = os.path.join(path, program)
            if is_exe(exe_file):
                return True

    return False

def create_replacement_function(vocab_model=None):
    REPLACEMENT=65533
    if vocab_model:
        vocab = set(map(int,open(vocab_model).read().split()))
        replace = lambda code: code if code in vocab else REPLACEMENT
    else:
        replace = lambda code: code
    return replace

def label(text, model, elman_model=None, vocab_model=None):
    """This function takes an input text, a wapiti model, an elman
    model and and vocab model, and calls wapiti to label the
    characters. It then returns the tokenized text."""
    # If text is empty, return empty string
    if not text:
        return ''
    def parse(line):
        fs = line.split()
        return (fs[0], fs[-1])
    replace = create_replacement_function()
    # write the input text to a string for wapiti
    if elman_model: # Add rnn features
        wapiti_input_lines = ['{0} {1} {2}\n'.format(ord(ch), ud.category(ch), features) for (ch, features) in zip(text, augment(text, elman_model, replace))]
    else:        # Just character codes and categories
        wapiti_input_lines = ['{0} {1}\n'.format(ord(ch), ud.category(ch)) for ch in text]
    wapiti_input_lines.append('\n')
    # call wapiti
    if not check_exec('wapiti'):
        raise RuntimeError("Unable to run wapiti")
    process = Popen(['wapiti', 'label', '-m', model], stdin=PIPE, stdout=PIPE, stderr=PIPE)
    for line in wapiti_input_lines:
        process.stdin.write(line)
    process.stdin.close()
    result = []
    while True:
        line = process.stdout.readline()
        if line.rstrip() == '':
            break
        else:
            result.append(parse(line.rstrip()))
    return result

def augment(text, elman_model, replace):
    """Takes an input text and an elman model and calls
    elman to output hidden layer activation for each character. It
    then extracts and returns binary features for use with wapiti."""
    
    # write the input text on a temporary file for elman
    tmp_input, tmp_input_file = mkstemp()
    fd = open(tmp_input_file, 'w')
    for ch in text:
        fd.write('{0} '.format(replace(ord(ch))))
    fd.write("\n")
    fd.close()
    os.close(tmp_input)

    # call elman
    if not check_exec('elman'):
        raise RuntimeError("Unable to run elman")
    process = Popen(['elman', '-rnnlm',  elman_model, '-hidden-feature-size', '10',\
                     '-print-hidden', '-test', tmp_input_file], \
                        stdout=PIPE, stderr=PIPE)
    output = process.communicate()[0][:-2]
    os.unlink(tmp_input_file)
    return output.split('\n')
    
def checks(model):
    errors = []
    
    # check for wapiti
    if not check_exec('wapiti'):
        errors.append('unable to run wapiti')
    
    # check for model file
    if not (os.path.isfile(model) and os.access(model, os.R_OK)):
        errors.append('unable to read model file {0}'.format(model))
    
    return len(errors) == 0, errors

def check_model_type(modeldir):
    """Checks if modeldir is a simple wapiti model or an elman+wapiti
    model or an elman+vocab+wapiti model. Returns paths to all model
    files (None for elman and vocab if not present)."""
    wapiti_model =  os.path.join(modeldir,"wapiti")
    elman_model = os.path.join(modeldir, "elman")
    vocab_model = os.path.join(modeldir, "vocab")
    # Wapiti model must exist and be readable
    if not os.access(wapiti_model, os.R_OK):
        raise RuntimeError("Cannot read {0}".format(wapiti_model))
    # If elman model exists, it must be readable
    if  os.path.isfile(elman_model) and \
        not os.access(elman_model, os.R_OK):
        raise RuntimeError("Cannot read {0}".format(elman_model))
    # If elman model does not exist, it won't be used so set it to None
    if not os.path.isfile(elman_model):
        elman_model = None
    # If vocab exists, it must be readable
    if  os.path.isfile(vocab_model) and \
        not os.access(vocab_model, os.R_OK):
        raise RuntimeError("Cannot read {0}".format(vocab_model))
    # If vocab model does not exist, it won't be used so set it to None
    if not os.path.isfile(vocab_model):
        vocab_model = None

    return ( wapiti_model, elman_model, vocab_model)
    
def output_labels(labeled, iob=False, first = True):
    """Output the tokenized text in either traditional format 
    (whitespace-separated tokens, one sentence per line) or in vertical 
    format with IOB tags"""
    out = codecs.getwriter("utf-8")(sys.stdout)
    if iob:
        for ch, label in labeled:
            out.write(u'{0}\t{1}\n'.format(int(ch), label))
        out.write(u'\n')
    else:
        first = first
        for ch, label in labeled:
            if label == 'S':
                if not first:
                    out.write(u'\n')
                first = False
            if label == 'T':
                out.write(u' ')
            if label in 'STI':
                out.write(unichr(int(ch)))  

if __name__ == '__main__':
    # parse command line options
    usage = "usage: %prog -m MODEL_DIR [options] < INPUT_FILE"
    parser = OptionParser(usage=usage)
    parser.add_option('-m', '--model', dest='model_dir',
            help='directory containing the model files to use')
    parser.add_option('-f', '--format', dest='format', default='',
            help='output format: "normal" (default) or "iob"')
    parser.add_option('-F', '--input-format', dest='input_format', default='',
            help='input format: "normal" (default) or "iob"')
    parser.add_option('-c', '--chunk-input', dest='chunk_input', default=0,
            type=int, metavar="CHUNK_SIZE_KB", help="chunk the input in n-KB-sized chunks to \
reduce Elephant's RAM usage. Max_RAM_usage_MB ~ 4*size_of_models_MB+0.7*chunk_size_KB")
    (options, args) = parser.parse_args()
    if not (options.model_dir):
        parser.print_help()
        sys.exit(1)
    if options.chunk_input > 0 and options.input_format == 'iob':
        print >> sys.stderr, 'ERROR: chunking input unsupported for IOB format'
        sys.exit(1)
    model_dir = options.model_dir
    iob =  options.format == 'iob'
    
    # check type of model
    wapiti_model, elman_model, vocab_model = check_model_type(options.model_dir)
    os.environ["PATH"] += os.pathsep + os.pathsep.join([os.path.dirname(os.path.abspath(__file__)),"ext"])
    
    # tokenize the input text
    if options.chunk_input > 0:
        inp = sys.stdin # codecs.getreader doesn't support sizehint in readlines()
        first = True
        # Cycle through input while it lasts, keep lines in input intact
        while True:
            text = unicode(''.join(inp.readlines(options.chunk_input*1000)), 'utf-8')
            if not text:
                break
            labeled = label(text, wapiti_model, elman_model, vocab_model)
            output_labels(labeled, iob=iob, first=first)
            first = False
    elif options.chunk_input < 0:
        parser.print_help()
        sys.exit(1)
    elif options.input_format == 'iob':
        def line_to_character(line):
            code, label = line.split()
            return unichr(int(code))
        for para in paras(sys.stdin):
            text = ''.join(line_to_character(line) for line in para)
            labeled = label(text, wapiti_model, elman_model, vocab_model)
            output_labels(labeled, iob=iob)
    else:
        inp = codecs.getreader("utf-8")(sys.stdin)
        labeled = label(inp.read(), wapiti_model, elman_model, vocab_model)
        output_labels(labeled, iob=iob)

